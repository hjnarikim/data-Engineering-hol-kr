# Lab3-1: Create Glue Crawler for initial full load data

PART A: Data Validation and ETL - Create Glue Crawler for initial full load data



초기에 Amazon DMS를 통해 Amazon S3 버킷으로 full load 되는 CSV데이터를 _Data Catalog_로 만들기 위해 Glue Crawler를 생성해 봅니다.
이번 실습은 아래 Architecture에서 **⓵번,⓶번**에 해당되는 단계이며, 이 단계를 완료하면 Amazon S3 버킷을 데이터 원본으로 사용하여 Glue Crawler로 Data Catalog 의 Database와 Table을 생성할 수 있습니다.<br>


![glue archi](../../images/1-2.png)<br>


#### 1. 먼저 [**AWS Glue**](https://console.aws.amazon.com/glue/home) 로 이동합니다.

![3-2](https://user-images.githubusercontent.com/105655711/197322091-1206248a-1e51-464f-8699-feb3d58c57f1.png)

#### 2. AWS Glue menu에서 Crawlers를 선택하세요.

![3-3](https://user-images.githubusercontent.com/105655711/197322192-f7d1fffd-ae03-4542-a950-771ef1693e54.png)

#### 3. Add crawler 클릭해서 crawler 이름을 다음 `glue-lab-crawler` 로 입력해 주세요.

#### 4. 필요한 경우 description을 입력하시고 Next를 클릭합니다.

![3-4](https://user-images.githubusercontent.com/105655711/197322549-43c6cf83-439a-4518-b3aa-f4d9bbdf9214.png)

#### 5. **Data stores**, **Crawl all folders**을 선택하고 Next를 클릭합니다.

![3-5](https://user-images.githubusercontent.com/105655711/197322794-99e0b1fd-9804-476c-913b-a5619db3cef3.png)

#### 6. **Add a data store** 페이지에서 다음을 선택해주세요.

```
a. Choose a data store에서 드롭다운 상자를 클릭하고 S3를 선택!
b. Crawl data in에서 Specified path를 선택!
c. Include path에는 앞서 CSV파일이 적재된 S3의 경로를 지정(ex> s3://xxx-dmslabs3bucket-xxx/tickets)
```

#### 7. 입력이 제대로 되었다면, 확인하시고 _Next_를 클릭해주세요.

![3-6](https://user-images.githubusercontent.com/105655711/197323218-92a189f4-fbd5-4468-9f72-a5f748377c79.png)

#### 8. Add another data store 에서 _No_ 를 선택하고 _Next_를 클릭합니다.

![3-7](https://user-images.githubusercontent.com/105655711/197323390-b930a39c-3906-4fdf-b5f7-6db58693cde9.png)

#### 9. Choose an IAM role 페이지에서 다음을 선택해주세요.

```
a. Choose an existing IAM role 선택!
b. For IAM role, select <stackname>-GlueLabRole-<RandomString> pre-created for you. 예시는 “dmslab-student-GlueLabRole-ZOQDII7JTBUM”
```

#### 10. 입력이 제대로 되었다면, 확인하시고 Next를 클릭해주세요.

![3-8](https://user-images.githubusercontent.com/105655711/197323529-0a57634d-1762-49b2-87c5-785e6771b7a9.png)

#### 11. Create a schedule for this crawler 페이지에서, Frequency(빈도)에 Run on demand를 선택해주시고 Next를 클릭합니다.

![3-9](https://user-images.githubusercontent.com/105655711/197323658-99c39a1c-f441-4257-bfd0-c6ea91a10567.png)

#### 12. Configure the crawler’s output 페이지에서, Add database 를 클릭해서 Glue Catalogue 대한 새 데이터베이스를 생성해 봅니다.

![3-10](https://user-images.githubusercontent.com/105655711/197323830-4218bed8-6cf5-4c0e-ab83-a7364fe99dee.png)

#### 13. Database Name은 `ticketdata` 를 입력하고 Create! 

![glue dbname](https://user-images.githubusercontent.com/105655711/197323929-4c4bf4a3-29a3-4539-bd04-fe9d7b964b98.png)


#### 14. _Prefix added to tables (optional)_ 는 빈칸으로 유지, _Configuration options (optional)_, 에서 _Add new columns only_ 만 선택해 주시고 Next를 클릭합니다.

![3-12](https://user-images.githubusercontent.com/105655711/197324429-f8c8ecfc-6c58-43a4-835c-89ef061092b3.png)

#### 15. 경로 및 데이터베이스 출력을 포함하는 Summary 페이지를 Review 하고 _Finish_를 클릭합니다. 이제 crawler를 실행할 준비가 되었습니다.

![3-13](https://user-images.githubusercontent.com/105655711/197324560-fe0549ac-3eac-4113-b6f8-07156ffcee79.png)

#### 16. crawler 이름 옆의 확인란을 선택하고 _Run crawler_ 버튼을 클릭합니다.

![3-14](https://user-images.githubusercontent.com/105655711/197324657-8ddd0537-d1ee-423c-bfaf-321e837e9fb4.png) crawler 상태가 Start → Stop 으로 변경되고 crawler 다시 Ready 상태로 돌아올 때까지 기다리면(이 프로세스는 몇 분 정도 소요됨) 15개의 테이블이 생성되었음을 알 수 있습니다. ![3-15](https://user-images.githubusercontent.com/105655711/197324661-77fb4c75-697a-4400-bec3-69535f97a918.png)

#### 17. AWS Glue 좌측의 탐색창에서, _Databases → Tables_ 선택하고. 또한 _ticketdata_ 데이터베이스를 클릭해서 테이블을 확인할 수 있습니다.

![3-16](https://user-images.githubusercontent.com/105655711/197324724-b8ce8369-1469-412c-9089-cea04ac8fd5a.png)

여기까지 _Create Glue Crawler for initial full load data_ 을 완료하셨습니다.\
이제 아래를 클릭하시어, Glue 를 통해 Crawlering 해온 데이터를 검증해보기 위한 Lab으로 이동합니다.\
\
[3-2.Data Validation and ETL - Data Validation Exercise](3-2.datavalidationexercise.md)
